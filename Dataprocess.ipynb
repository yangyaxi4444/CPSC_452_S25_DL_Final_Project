{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGvCvk2LS97d"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load core ICU info\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load ICU stays\n",
        "icustays = pd.read_csv(\"icustays.csv.gz\", compression=\"gzip\", parse_dates=[\"intime\", \"outtime\"])\n",
        "icu_meta = icustays[[\"subject_id\", \"hadm_id\", \"stay_id\", \"intime\", \"first_careunit\", \"los\"]]\n",
        "\n",
        "# Load hospital admission info and assign mortality label\n",
        "admissions = pd.read_csv(\"admissions.csv.gz\", compression='gzip')\n",
        "df_mortality = admissions.merge(icustays, on=[\"subject_id\", \"hadm_id\"])\n",
        "df_mortality[\"mortality_label\"] = df_mortality[\"hospital_expire_flag\"]\n",
        "df_mortality = df_mortality[[\"subject_id\", \"hadm_id\", \"stay_id\", \"mortality_label\"]]\n",
        "\n",
        "# Step 2: Generate structured_note from icustays\n",
        "icu_meta[\"structured_note\"] = icu_meta.apply(\n",
        "    lambda row: f\"The patient was admitted to the {row['first_careunit']} and stayed for {round(row['los'], 1)} days.\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Step 3: Load procedure events\n",
        "procedures = pd.read_csv(\"procedureevents.csv.gz\", compression=\"gzip\", parse_dates=[\"starttime\"])\n",
        "\n",
        "# Step 4: Load d_items to decode procedure names\n",
        "items = pd.read_csv(\"d_items.csv.gz\", compression=\"gzip\")\n",
        "proc_map = items.set_index(\"itemid\")[\"label\"].to_dict()\n",
        "procedures[\"proc_name\"] = procedures[\"itemid\"].map(proc_map)\n",
        "\n",
        "# Step 5: Merge intime into procedures\n",
        "proc_with_time = procedures.merge(icustays[[\"subject_id\", \"stay_id\", \"intime\"]], on=[\"subject_id\", \"stay_id\"], how=\"left\")\n",
        "\n",
        "# Keep only those within 24h of ICU admission\n",
        "proc_with_time = proc_with_time[\n",
        "    (proc_with_time[\"starttime\"] >= proc_with_time[\"intime\"])\n",
        "    & (proc_with_time[\"starttime\"] <= proc_with_time[\"intime\"] + pd.Timedelta(hours=24))\n",
        "]\n",
        "\n",
        "# Step 6: Aggregate procedure descriptions\n",
        "proc_summaries = proc_with_time.groupby([\"subject_id\", \"hadm_id\", \"stay_id\"])['proc_name'].apply(lambda x: list(set(x.dropna()))).reset_index()\n",
        "\n",
        "# Turn procedure list into sentence\n",
        "proc_summaries[\"procedure_note\"] = proc_summaries[\"proc_name\"].apply(\n",
        "    lambda x: \"The patient received the following procedures within the first 24 hours: \" + \", \".join(x) + \".\" if x else \"\",\n",
        ")\n",
        "\n",
        "# Step 7: Merge all back together\n",
        "icu_enriched = icu_meta.merge(proc_summaries[[\"subject_id\", \"hadm_id\", \"stay_id\", \"procedure_note\"]],\n",
        "                              on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"left\")\n",
        "\n",
        "icu_enriched[\"procedure_note\"] = icu_enriched[\"procedure_note\"].fillna(\"\")\n",
        "\n",
        "# Step 8: Add discharge notes\n",
        "print(\"Loading discharge notes...\")\n",
        "discharge = pd.read_csv(\"discharge.csv.gz\", compression='gzip')\n",
        "discharge_latest = discharge.sort_values(\"charttime\").drop_duplicates(\"hadm_id\", keep=\"last\")\n",
        "discharge_latest[\"text_note\"] = discharge_latest[\"text\"].str.replace(r\"\\[\\*\\*.*?\\*\\*\\]\", \"\", regex=True)\n",
        "df_mortality = df_mortality.merge(discharge_latest[[\"hadm_id\", \"text_note\"]], on=\"hadm_id\", how=\"inner\")\n",
        "\n",
        "# Step 9: Add lab-based vitals from labevents\n",
        "print(\"Loading lab events for vitals...\")\n",
        "labs = pd.read_csv(\"labevents.csv.gz\", compression=\"gzip\", parse_dates=[\"charttime\"])\n",
        "\n",
        "# Example lab-based vital signs and proxies\n",
        "vital_lab_items = {\n",
        "    50912: \"glucose\",\n",
        "    50983: \"sodium\",\n",
        "    50822: \"potassium\",\n",
        "    51006: \"bun\",\n",
        "    50971: \"creatinine\",\n",
        "    50868: \"calcium\",\n",
        "    50862: \"chloride\",\n",
        "    50902: \"magnesium\",\n",
        "    50809: \"albumin\",\n",
        "    50820: \"bilirubin_total\"\n",
        "}\n",
        "\n",
        "labs = labs[labs[\"itemid\"].isin(vital_lab_items.keys())]\n",
        "labs[\"vital_label\"] = labs[\"itemid\"].map(vital_lab_items)\n",
        "# Clean value column to keep only numeric values\n",
        "labs[\"value\"] = pd.to_numeric(labs[\"value\"], errors=\"coerce\")\n",
        "labs = labs.dropna(subset=[\"value\"])\n",
        "\n",
        "# Now safely aggregate\n",
        "lab_summary = labs.groupby([\"subject_id\", \"hadm_id\", \"itemid\"]).agg({\"value\": \"median\"}).reset_index()\n",
        "\n",
        "lab_summary[\"vital_label\"] = lab_summary[\"itemid\"].map(vital_lab_items)\n",
        "\n",
        "lab_pivot = lab_summary.pivot_table(index=[\"subject_id\", \"hadm_id\"], columns=\"vital_label\", values=\"value\").reset_index()\n",
        "lab_pivot.columns.name = None\n",
        "\n",
        "# Step 10: Add CXR image path\n",
        "print(\"Loading CXR metadata...\")\n",
        "study_df = pd.read_csv(\"cxr-study-list.csv.gz\", compression=\"gzip\")\n",
        "record_df = pd.read_csv(\"cxr-record-list.csv.gz\", compression=\"gzip\")\n",
        "cxr_meta = study_df.merge(record_df, on=[\"subject_id\", \"study_id\"], how=\"inner\")\n",
        "cxr_meta[\"image_path\"] = cxr_meta[\"dicom_id\"].apply(\n",
        "    lambda x: f\"mimic-cxr/files/p{str(x)[:2]}/p{str(x)}/s{x}/{x}.jpg.gz\"\n",
        ")\n",
        "cxr_meta_dedup = cxr_meta.sort_values(\"study_id\").drop_duplicates(\"subject_id\", keep=\"first\")\n",
        "\n",
        "# Step 11: Merge everything into one final dataset\n",
        "df_full = df_mortality.merge(icu_enriched, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"left\")\n",
        "df_full = df_full.merge(lab_pivot, on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n",
        "df_full = df_full.merge(cxr_meta_dedup[[\"subject_id\", \"image_path\"]], on=\"subject_id\", how=\"inner\")\n",
        "df_full[\"combined_note\"] = df_full[\"procedure_note\"].fillna(\"\") + \" \" + df_full[\"structured_note\"].fillna(\"\") + \" \" + df_full[\"text_note\"].fillna(\"\")\n",
        "\n",
        "# Save final result\n",
        "df_full.to_csv(\"final_multimodal_dataset.csv\", index=False)\n",
        "\n",
        "# Output summary\n",
        "print(\"\\nFinal dataset columns:\")\n",
        "print(list(df_full.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any column with too much missing value\n",
        "clean_dataset = filtered_dataset.drop(columns=['potassium', 'albumin', 'chloride', 'bilirubin_total'])\n",
        "\n",
        "# Drop all rows with any missing values\n",
        "clean_dataset = clean_dataset.dropna()\n",
        "\n",
        "# Optional: check new size\n",
        "print(f\"Remaining rows after dropping missing values: {len(clean_dataset)}\")\n"
      ],
      "metadata": {
        "id": "ZkK3ej91TNCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add image information"
      ],
      "metadata": {
        "id": "nb9_Ylh2TRqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cxr_records = pd.read_csv(\"data_process/cxr-record-list.csv.gz\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "JA0opawDTQcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplicate to get one dicom_id + study_id per subject\n",
        "cxr_records_dedup = cxr_records.sort_values(\"study_id\").drop_duplicates(\"subject_id\", keep=\"first\")\n",
        "\n",
        "# Merge to bring in study_id and dicom_id\n",
        "df_merged = clean_dataset.merge(cxr_records_dedup[[\"subject_id\", \"study_id\", \"dicom_id\"]], on=\"subject_id\", how=\"inner\")\n",
        "df_merged[\"image_path\"] = df_merged.apply(\n",
        "    lambda row: f\"files/p{str(row['subject_id']).zfill(8)[:2]}/p{str(row['subject_id']).zfill(8)}/s{int(row['study_id'])}/{row['dicom_id']}.dcm\",\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "hdGeJqTnTUZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged\n",
        "print(df_merged.columns.tolist())"
      ],
      "metadata": {
        "id": "uhghtHp0TVbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.to_csv(\"final_image.csv\", index=False)"
      ],
      "metadata": {
        "id": "Rk5PJjwpTXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = df_merged.drop(columns=['study_id', 'dicom_id', 'subject_id', 'hadm_id', 'stay_id'])"
      ],
      "metadata": {
        "id": "lzCdtbygTY6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_csv(\"final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6QbFVV1gTcYm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}