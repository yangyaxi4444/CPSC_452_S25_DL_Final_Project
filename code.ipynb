{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUa5NjqxvISxXzsKYy8Cen"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## baseline plain"],"metadata":{"id":"VO-8OjjqS3RT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0J4WHBOOSyqY"},"outputs":[],"source":["# bert_baseline_with_full_metrics.py\n","\n","import math\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","from sklearn.metrics import (\n","    roc_auc_score, accuracy_score, precision_score, recall_score,\n","    f1_score, matthews_corrcoef, confusion_matrix\n",")\n","import matplotlib.pyplot as plt\n","import gc\n","\n","class ICUTextDataset(Dataset):\n","    def __init__(self, csv_path, tokenizer_name, max_length, mode='both'):\n","        self.data = pd.read_csv(csv_path).reset_index(drop=True)\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        self.max_length = max_length\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row      = self.data.iloc[idx]\n","        text     = str(row['text_note'])\n","        combined = str(row['combined_note'])\n","        if   self.mode=='text_only':    full_text = text\n","        elif self.mode=='combined_only': full_text = combined\n","        else:                           full_text = text + ' ' + combined\n","\n","        enc = self.tokenizer(\n","            full_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids':      enc['input_ids'].squeeze(0),\n","            'attention_mask': enc['attention_mask'].squeeze(0),\n","            'label':          torch.tensor(row['mortality_label'], dtype=torch.float32)\n","        }\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self, encoder_name, hidden_dim=768):\n","        super().__init__()\n","        self.encoder = AutoModel.from_pretrained(encoder_name)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(hidden_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_emb = out.last_hidden_state[:,0,:]\n","        return self.classifier(cls_emb)\n","\n","def compute_metrics(probs, labels):\n","    # determine threshold to match positive rate\n","    P = int(sum(labels))\n","    if P > 0:\n","        thr = sorted(probs, reverse=True)[P-1]\n","    else:\n","        thr = 1.0\n","    preds = [1 if p>=thr else 0 for p in probs]\n","    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n","    return {\n","        'auc': roc_auc_score(labels, probs),\n","        'accuracy': accuracy_score(labels, preds),\n","        'precision': precision_score(labels, preds, zero_division=0),\n","        'recall': recall_score(labels, preds, zero_division=0),\n","        'f1': f1_score(labels, preds, zero_division=0),\n","        'mcc': matthews_corrcoef(labels, preds),\n","        'specificity': tn/(tn+fp) if (tn+fp)>0 else 0,\n","        'npv': tn/(tn+fn) if (tn+fn)>0 else 0,\n","        'threshold': thr\n","    }\n","\n","def train_epoch(model, loader, device, loss_fn, optimizer):\n","    model.train()\n","    total_loss = 0\n","    all_probs, all_labels = [], []\n","    for batch in tqdm(loader, desc=\"Train\", leave=False):\n","        ids    = batch['input_ids'].to(device)\n","        mask   = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device).unsqueeze(1)\n","        logits = model(ids, mask)\n","        loss   = loss_fn(logits, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        probs = torch.sigmoid(logits).cpu().squeeze().tolist()\n","        all_probs.extend(probs if isinstance(probs, list) else [probs])\n","        all_labels.extend(labels.cpu().squeeze().tolist())\n","    metrics = compute_metrics(all_probs, all_labels)\n","    metrics['loss'] = total_loss/len(loader)\n","    return metrics\n","\n","def eval_epoch(model, loader, device, loss_fn):\n","    model.eval()\n","    total_loss = 0\n","    all_probs, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Eval\", leave=False):\n","            ids    = batch['input_ids'].to(device)\n","            mask   = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device).unsqueeze(1)\n","            logits = model(ids, mask)\n","            loss   = loss_fn(logits, labels)\n","            total_loss += loss.item()\n","            probs = torch.sigmoid(logits).cpu().squeeze().tolist()\n","            all_probs.extend(probs if isinstance(probs, list) else [probs])\n","            all_labels.extend(labels.cpu().squeeze().tolist())\n","    metrics = compute_metrics(all_probs, all_labels)\n","    metrics['loss'] = total_loss/len(loader)\n","    return metrics\n","\n","def run_experiment(model_name=\"bert-base-uncased\", max_length=512, mode_label=\"both\"):\n","    ds = ICUTextDataset(\"final.csv\", model_name, max_length, mode_label)\n","    n  = len(ds)\n","    train_n = int(0.8*n)\n","    train_ds, val_ds = random_split(ds, [train_n, n-train_n])\n","    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n","    val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model  = BERTClassifier(model_name).to(device)\n","    train_labels = [ds[i]['label'].item() for i in train_ds.indices]\n","    neg, pos = train_labels.count(0), train_labels.count(1)\n","    pos_weight = torch.tensor([(neg/pos) if pos>0 else 1.0], device=device)\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","\n","    epochs = 100\n","    history = {k: [] for k in ['loss','auc','accuracy','precision','recall','f1','mcc','specificity','npv','threshold']}\n","    history_val = {k: [] for k in history}\n","\n","    for epoch in range(1, epochs+1):\n","        print(f\"\\nEpoch {epoch}/{epochs}\")\n","        m_tr = train_epoch(model, train_loader, device, loss_fn, optimizer)\n","        m_val= eval_epoch (model, val_loader,   device, loss_fn)\n","        for k in history:\n","            history[k].append(m_tr[k])\n","            history_val[k].append(m_val[k])\n","        print(\" Train:\", {k: f\"{m_tr[k]:.4f}\" for k in ['loss','auc','accuracy','precision','recall','f1','mcc']})\n","        print(\" Val:  \", {k: f\"{m_val[k]:.4f}\" for k in ['loss','auc','accuracy','precision','recall','f1','mcc']})\n","\n","    # Plotting selected metrics\n","    plt.figure(figsize=(14,10))\n","    metrics_to_plot = ['loss','auc','accuracy','precision','recall','f1','mcc']\n","    for i, metric in enumerate(metrics_to_plot,1):\n","        plt.subplot(3,3,i)\n","        plt.plot(history[metric], label=f\"Train {metric}\")\n","        plt.plot(history_val[metric], label=f\"Val   {metric}\")\n","        plt.title(metric)\n","        plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # cleanup\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","run_experiment()\n","\n","\n"]},{"cell_type":"markdown","source":["## base line + keyword"],"metadata":{"id":"twhWuZ0aS7XK"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\n","import re\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# ──── DATASET (TEXT ONLY) ──────────────────────────────────────────────────\n","class ClinicalNotesDataset(Dataset):\n","    def __init__(self, csv_path, tokenizer_name, max_length=512, mode='combined'):\n","        \"\"\"\n","        Args:\n","            mode: 'text' (original note), 'combined' (all notes), or 'discharge' (discharge summary only)\n","        \"\"\"\n","        self.data = pd.read_csv(csv_path)\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        self.max_length = max_length\n","        self.mode = mode\n","\n","        # Clean text during initialization\n","        self.data['cleaned_text'] = self.data['text_note'].apply(self._clean_text)\n","        self.data['cleaned_combined'] = self.data['combined_note'].apply(self._clean_text)\n","\n","    def _clean_text(self, text):\n","        \"\"\"Remove clinical note artifacts and truncate intelligently\"\"\"\n","        text = str(text)\n","        # 1. Remove de-identification markers\n","        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', text)\n","        # 2. Remove multiple newlines and whitespace\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","        # 3. Keep last 2048 characters (prioritize recent info)\n","        return text[-2048:] if len(text) > 2048 else text\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","\n","        # Select text based on mode\n","        if self.mode == 'text':\n","            text = row['cleaned_text']\n","        elif self.mode == 'discharge':\n","            text = self._extract_discharge_section(row['cleaned_combined'])\n","        else:  # combined\n","            text = row['cleaned_combined']\n","\n","        # Tokenize\n","        inputs = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].squeeze(0),\n","            'attention_mask': inputs['attention_mask'].squeeze(0),\n","            'label': torch.tensor(row['mortality_label'], dtype=torch.float32)\n","        }\n","\n","    def _extract_discharge_section(self, text):\n","        \"\"\"Extract discharge summary section if available\"\"\"\n","        match = re.search(r'DISCHARGE SUMMARY:(.*?)(?=\\n[A-Z]{2,}:|$)', text, re.IGNORECASE|re.DOTALL)\n","        return match.group(1).strip() if match else text\n","\n","# ──── MODEL (BERT ONLY) ───────────────────────────────────────────────────\n","class BERTMortalityPredictor(nn.Module):\n","    def __init__(self, model_name='emilyalsentzer/Bio_ClinicalBERT', dropout_rate=0.2):\n","        super().__init__()\n","        self.bert = AutoModel.from_pretrained(model_name)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(768, 256),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n","        pooled = self.dropout(pooled)\n","        return self.classifier(pooled)\n","\n","# ──── TRAINING UTILITIES ──────────────────────────────────────────────────\n","def train_epoch(model, dataloader, device, optimizer, loss_fn):\n","    model.train()\n","    total_loss = 0\n","    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n","\n","    for batch in pbar:\n","        optimizer.zero_grad()\n","\n","        inputs = batch['input_ids'].to(device)\n","        masks = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device).unsqueeze(1)\n","\n","        outputs = model(inputs, masks)\n","        loss = loss_fn(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        pbar.set_postfix({'loss': loss.item()})\n","\n","    return total_loss / len(dataloader)\n","\n","def evaluate(model, dataloader, device, threshold=None):\n","    model.eval()\n","    preds, labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            inputs = batch['input_ids'].to(device)\n","            masks = batch['attention_mask'].to(device)\n","\n","            outputs = model(inputs, masks).squeeze()\n","            probs = torch.sigmoid(outputs).cpu().numpy()\n","            preds.extend(probs)\n","            labels.extend(batch['label'].cpu().numpy())\n","\n","    # Dynamic thresholding if not specified\n","    if threshold is None:\n","        threshold = np.percentile(preds, 100 * (1 - np.mean(labels)))\n","\n","    bin_preds = (np.array(preds) > threshold).astype(int)\n","\n","    print(f\"\\nEvaluation (Threshold={threshold:.3f})\")\n","    print(f\"AUC: {roc_auc_score(labels, preds):.4f}\")\n","    print(f\"F1: {f1_score(labels, bin_preds):.4f}\")\n","    print(classification_report(labels, bin_preds))\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(labels, bin_preds))\n","\n","    return {\n","        'auc': roc_auc_score(labels, preds),\n","        'f1': f1_score(labels, bin_preds),\n","        'threshold': threshold\n","    }\n","\n","# ──── MAIN EXPERIMENT ─────────────────────────────────────────────────────\n","def run_experiment():\n","    # Config\n","    MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"  # Clinical BERT variant\n","    MAX_LENGTH = 512\n","    BATCH_SIZE = 16\n","    EPOCHS = 10\n","    MODE = 'combined'  # 'text', 'combined', or 'discharge'\n","\n","    # Setup\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # Data\n","    dataset = ClinicalNotesDataset(\"final.csv\", MODEL_NAME, MAX_LENGTH, mode=MODE)\n","\n","    # Stratified split\n","    train_idx, val_idx = train_test_split(\n","        range(len(dataset)),\n","        test_size=0.2,\n","        stratify=dataset.data['mortality_label'],\n","        random_state=42\n","    )\n","    train_ds = torch.utils.data.Subset(dataset, train_idx)\n","    val_ds = torch.utils.data.Subset(dataset, val_idx)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n","    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, pin_memory=True)\n","\n","    # Model\n","    model = BERTMortalityPredictor(MODEL_NAME).to(device)\n","\n","    # Handle class imbalance\n","    pos_weight = torch.tensor([\n","        (len(train_ds) - sum(dataset.data.iloc[train_idx]['mortality_label'])) /\n","        sum(dataset.data.iloc[train_idx]['mortality_label'])\n","    ]).to(device)\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","\n","    # Optimizer\n","    optimizer = torch.optim.AdamW([\n","        {'params': model.bert.parameters(), 'lr': 2e-5},\n","        {'params': model.classifier.parameters(), 'lr': 1e-4}\n","    ], weight_decay=1e-4)\n","\n","    # Training loop\n","    best_f1 = 0\n","    for epoch in range(EPOCHS):\n","        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n","\n","        # Train\n","        train_loss = train_epoch(model, train_loader, device, optimizer, loss_fn)\n","\n","        # Evaluate\n","        val_metrics = evaluate(model, val_loader, device)\n","\n","        # Save best model\n","        if val_metrics['f1'] > best_f1:\n","            best_f1 = val_metrics['f1']\n","            torch.save(model.state_dict(), f\"best_bert_{MODE}.pt\")\n","            print(f\"New best model saved (F1={best_f1:.4f})\")\n","\n","if __name__ == \"__main__\":\n","    run_experiment()"],"metadata":{"id":"4F80GleTTD7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Two feature Plain"],"metadata":{"id":"cSag1M0bTEe7"}},{"cell_type":"code","source":["# bert_baseline_struct_full_metrics_updated.py\n","\n","import math\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.preprocessing import StandardScaler\n","from tqdm import tqdm\n","from sklearn.metrics import (\n","    roc_auc_score, accuracy_score, precision_score, recall_score,\n","    f1_score, matthews_corrcoef, confusion_matrix\n",")\n","import matplotlib.pyplot as plt\n","import gc\n","\n","# ─── DATASET WITH STRUCTURED FEATURES ─────────────────────────────────────\n","class ICUTextStructDataset(Dataset):\n","    def __init__(self, csv_path, tokenizer_name, max_length, mode='both'):\n","        df = pd.read_csv(csv_path).reset_index(drop=True)\n","        # structured columns to include\n","        self.struct_cols = ['bun','calcium','creatinine','glucose','magnesium','sodium','los']\n","        # coerce to numeric, fill missing\n","        df[self.struct_cols] = (\n","            df[self.struct_cols]\n","              .apply(pd.to_numeric, errors='coerce')\n","              .fillna(0.0)\n","        )\n","        # standardize structured features\n","        scaler = StandardScaler()\n","        df[self.struct_cols] = scaler.fit_transform(df[self.struct_cols])\n","        self.data = df\n","\n","        # tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        self.max_length = max_length\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        text = str(row['text_note'])\n","        combined = str(row['combined_note'])\n","        if self.mode == 'text_only':\n","            full_text = text\n","        elif self.mode == 'combined_only':\n","            full_text = combined\n","        else:\n","            full_text = text + ' ' + combined\n","\n","        enc = self.tokenizer(\n","            full_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        # build structured features list explicitly as floats\n","        struct_list = [float(row[c]) for c in self.struct_cols]\n","        struct_feats = torch.tensor(struct_list, dtype=torch.float32)\n","        label = torch.tensor(row['mortality_label'], dtype=torch.float32)\n","        return {\n","            'input_ids':      enc['input_ids'].squeeze(0),\n","            'attention_mask': enc['attention_mask'].squeeze(0),\n","            'struct_feats':   struct_feats,\n","            'label':          label\n","        }\n","\n","# ─── MODEL WITH STRUCTURED MLP ────────────────────────────────────────────\n","class BERTWithStruct(nn.Module):\n","    def __init__(self, encoder_name, struct_dim=7, bert_dim=768):\n","        super().__init__()\n","        self.encoder = AutoModel.from_pretrained(encoder_name)\n","        # MLP for structured features\n","        self.struct_mlp = nn.Sequential(\n","            nn.Linear(struct_dim, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 32)\n","        )\n","        # fusion classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(bert_dim + 32, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, input_ids, attention_mask, struct_feats):\n","        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_emb = out.last_hidden_state[:, 0, :]       # (B, bert_dim)\n","        struct_emb = self.struct_mlp(struct_feats)     # (B, 32)\n","        x = torch.cat([cls_emb, struct_emb], dim=1)    # (B, bert_dim + 32)\n","        return self.classifier(x)                      # (B,1) logits\n","\n","# ─── METRICS CALCULATION ─────────────────────────────────────────────────\n","def compute_metrics(probs, labels):\n","    # calibrate threshold to match positive rate\n","    P = int(sum(labels))\n","    thr = sorted(probs, reverse=True)[P-1] if P > 0 else 1.0\n","    preds = [1 if p>=thr else 0 for p in probs]\n","    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n","    return {\n","        'auc': roc_auc_score(labels, probs),\n","        'accuracy': accuracy_score(labels, preds),\n","        'precision': precision_score(labels, preds, zero_division=0),\n","        'recall': recall_score(labels, preds, zero_division=0),\n","        'f1': f1_score(labels, preds, zero_division=0),\n","        'mcc': matthews_corrcoef(labels, preds),\n","        'specificity': tn/(tn+fp) if (tn+fp)>0 else 0.0,\n","        'npv': tn/(tn+fn) if (tn+fn)>0 else 0.0,\n","        'threshold': thr\n","    }\n","\n","# ─── TRAIN & EVAL LOOPS ──────────────────────────────────────────────────\n","def train_epoch(model, loader, device, loss_fn, optimizer):\n","    model.train()\n","    total_loss = 0.0\n","    all_probs, all_labels = [], []\n","    for batch in tqdm(loader, desc=\"Train\", leave=False):\n","        ids    = batch['input_ids'].to(device)\n","        mask   = batch['attention_mask'].to(device)\n","        struct = batch['struct_feats'].to(device)\n","        labels = batch['label'].to(device).unsqueeze(1)\n","\n","        logits = model(ids, mask, struct)\n","        loss   = loss_fn(logits, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        probs = torch.sigmoid(logits).cpu().squeeze().tolist()\n","        all_probs.extend(probs if isinstance(probs, list) else [probs])\n","        all_labels.extend(labels.cpu().squeeze().tolist())\n","\n","    metrics = compute_metrics(all_probs, all_labels)\n","    metrics['loss'] = total_loss / len(loader)\n","    return metrics\n","\n","def eval_epoch(model, loader, device, loss_fn):\n","    model.eval()\n","    total_loss = 0.0\n","    all_probs, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Eval\", leave=False):\n","            ids    = batch['input_ids'].to(device)\n","            mask   = batch['attention_mask'].to(device)\n","            struct = batch['struct_feats'].to(device)\n","            labels = batch['label'].to(device).unsqueeze(1)\n","\n","            logits = model(ids, mask, struct)\n","            loss   = loss_fn(logits, labels)\n","            total_loss += loss.item()\n","\n","            probs = torch.sigmoid(logits).cpu().squeeze().tolist()\n","            all_probs.extend(probs if isinstance(probs, list) else [probs])\n","            all_labels.extend(labels.cpu().squeeze().tolist())\n","\n","    metrics = compute_metrics(all_probs, all_labels)\n","    metrics['loss'] = total_loss / len(loader)\n","    return metrics\n","\n","def run_experiment(model_name=\"bert-base-uncased\", max_length=512, mode_label=\"both\n","\n","    ds = ICUTextStructDataset(\"final.csv\", model_name, max_length, mode_label)\n","    n = len(ds)\n","    train_n = int(0.8 * n)\n","    train_ds, val_ds = random_split(ds, [train_n, n - train_n])\n","    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n","    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = BERTWithStruct(model_name).to(device)\n","    # compute pos_weight for imbalance\n","    train_labels = [ds[i]['label'].item() for i in train_ds.indices]\n","    neg, pos = train_labels.count(0), train_labels.count(1)\n","    pos_weight = torch.tensor([(neg/pos) if pos>0 else 1.0], device=device)\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","    epochs = 100\n","    keys = ['loss','auc','accuracy','precision','recall','f1','mcc','specificity','\n","    hist_tr = {k: [] for k in keys}\n","    hist_val= {k: [] for k in keys}\n","    for epoch in range(1, epochs+1):\n","        print(f\"\\nEpoch {epoch}/{epochs}\")\n","        m_tr = train_epoch(model, train_loader, device, loss_fn, optimizer)\n","        m_val = eval_epoch(model, val_loader, device, loss_fn)\n","        for k in keys:\n","            hist_tr[k].append(m_tr[k])\n","            hist_val[k].append(m_val[k])\n","        # print full metrics\n","        summary_tr = {k: f\"{m_tr[k]:.4f}\" for k in keys}\n","        summary_val= {k: f\"{m_val[k]:.4f}\" for k in keys}\n","        print(\" Train:\", summary_tr)\n","        print(\" Val: \", summary_val)\n","        # plotting selected metrics\n","    plt.figure(figsize=(14,10))\n","    for i, metric in enumerate(['auc','accuracy','precision','recall','f1','mcc'],\n","        plt.subplot(3,3,i)\n","        plt.plot(hist_tr[metric], label=\"Train\")\n","        plt.plot(hist_val[metric], label=\"Val\")\n","        plt.title(metric)\n","        plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","    # cleanup\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","run_experiment()"],"metadata":{"id":"S9ik9Fx7TF9J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Two feature with keyword"],"metadata":{"id":"narMUsu8TtA7"}},{"cell_type":"code","source":["# Fixed imports (added missing ones)\n","import re\n","import math\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from tqdm import tqdm\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.cuda.amp\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# 1. Enhanced Data Processing - Removed image feature handling\n","def load_and_preprocess_data(csv_path):\n","    df = pd.read_pickle(csv_path)\n","\n","    structured_cols = ['bun', 'calcium', 'creatinine', 'glucose', 'magnesium', 'sodium']\n","\n","    # Improved missing value handling\n","    for col in structured_cols:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","        df[f'{col}_missing'] = df[col].isna().astype(float)  # Missingness flags\n","        df[col] = df[col].fillna(0)\n","\n","    scaler = StandardScaler()\n","    df[structured_cols] = scaler.fit_transform(df[structured_cols])\n","\n","    # Add temporal features if available\n","    if 'charttime' in df:\n","        try:\n","            df['hour_of_day'] = pd.to_datetime(df['charttime']).dt.hour\n","            structured_cols.append('hour_of_day')\n","        except:\n","            print(\"Could not parse charttime for hour_of_day feature\")\n","\n","    df['mortality_label'] = pd.to_numeric(df['mortality_label'], errors='coerce').astype(int)\n","\n","    class_counts = df['mortality_label'].value_counts()\n","    print(f\"Class distribution: {class_counts.to_dict()}\")\n","    print(f\"Percentage of positive samples: {class_counts.get(1, 0) / len(df) * 100:.2f}%\")\n","\n","    return df, structured_cols + [f'{col}_missing' for col in structured_cols]\n","\n","# 2. Enhanced Dataset Class - Removed image features\n","class MultimodalDataset(Dataset):\n","    def __init__(self, df, structured_cols, tokenizer, max_length=512):\n","        self.df = df\n","        self.structured_cols = structured_cols\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.section_pattern = re.compile(\n","            r'(IMPRESSION|ASSESSMENT|DIAGNOSIS|DISCHARGE SUMMARY):(.*?)(?=\\n[A-Z]{2,}:|$)',\n","            re.IGNORECASE | re.DOTALL\n","        )\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _clean_text(self, text):\n","        text = str(text)\n","        # 1. Extract clinical sections\n","        sections = self.section_pattern.findall(text)\n","        clean_text = ' '.join([s[1].strip() for s in sections]) if sections else text\n","\n","        # 2. Remove boilerplate and de-id artifacts\n","        clean_text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', clean_text)  # Remove [** **] patterns\n","        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()  # Normalize whitespace\n","\n","        # 3. Prioritize recent info (last 2048 chars if long)\n","        return clean_text[-2048:] if len(clean_text) > 2048 else clean_text\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Text processing\n","        clean_text = self._clean_text(row['combined_note'])\n","        text_feat = self.tokenizer(\n","            clean_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Structured data with missingness flags\n","        struct_data = [float(row[col]) for col in self.structured_cols]\n","        struct_feat = torch.tensor(struct_data, dtype=torch.float32)\n","\n","        label = torch.tensor(float(row['mortality_label']), dtype=torch.float32)\n","\n","        return {\n","            'input_ids': text_feat['input_ids'].squeeze(0),\n","            'attention_mask': text_feat['attention_mask'].squeeze(0),\n","            'struct_feat': struct_feat,\n","            'label': label\n","        }\n","\n","# 3. Model Architecture - Removed image components\n","class TextStructFusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', struct_dim=12, hidden_dim=256, dropout_rate=0.3):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        self.bert_hidden_size = self.bert.config.hidden_size\n","\n","        # Structured data projection\n","        self.struct_projection = nn.Sequential(\n","            nn.Linear(struct_dim, hidden_dim*2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim*2, hidden_dim)\n","        )\n","\n","        # Attention mechanism now only between text and structured data\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.bert_hidden_size + hidden_dim, hidden_dim),\n","            nn.Tanh(),\n","            nn.Linear(hidden_dim, 2),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.bert_hidden_size + hidden_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","    def forward(self, input_ids, attention_mask, struct_feat):\n","        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        text_embed = bert_outputs.last_hidden_state[:, 0, :]\n","\n","        struct_proj = self.struct_projection(struct_feat)\n","\n","        combined = torch.cat([text_embed, struct_proj], dim=1)\n","        attention_weights = self.attention(combined)\n","\n","        text_embed = text_embed * attention_weights[:, 0].unsqueeze(1)\n","        struct_proj = struct_proj * attention_weights[:, 1].unsqueeze(1)\n","\n","        fused = torch.cat([text_embed, struct_proj], dim=1)\n","        return self.classifier(fused).squeeze()\n","\n","# [Rest of the code remains the same, except for model instantiation]\n","# 4. Training Utilities (unchanged)\n","def compute_metrics(y_true, y_pred, threshold=0.5):\n","    y_pred = np.array(y_pred)\n","    y_pred_bin = (y_pred >= threshold).astype(int)\n","    return {\n","        'auc': roc_auc_score(y_true, y_pred),\n","        'accuracy': accuracy_score(y_true, y_pred_bin),\n","        'precision': precision_score(y_true, y_pred_bin, zero_division=0),\n","        'recall': recall_score(y_true, y_pred_bin, zero_division=0),\n","        'f1': f1_score(y_true, y_pred_bin, zero_division=0)\n","    }\n","\n","def find_optimal_threshold(y_true, y_pred):\n","    thresholds = np.arange(0.1, 0.9, 0.05)\n","    best_threshold = 0.5\n","    best_f1 = 0\n","    for threshold in thresholds:\n","        f1 = f1_score(y_true, (y_pred >= threshold).astype(int), zero_division=0)\n","        if f1 > best_f1:\n","            best_f1 = f1\n","            best_threshold = threshold\n","    return best_threshold\n","\n","# 5. Training Loop (modified to remove image features)\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device='cuda', clip_value=1.0):\n","    model = model.to(device)\n","    history = {'train': [], 'val': []}\n","    best_f1 = 0.0\n","    patience = 5\n","    epochs_without_improvement = 0\n","    scaler = torch.cuda.amp.GradScaler()\n","    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.1)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_preds, train_labels = [], []\n","        train_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            optimizer.zero_grad()\n","\n","            with torch.cuda.amp.autocast():\n","                outputs = model(\n","                    batch['input_ids'].to(device),\n","                    batch['attention_mask'].to(device),\n","                    batch['struct_feat'].to(device)\n","                )\n","                loss = criterion(outputs, batch['label'].to(device))\n","\n","            scaler.scale(loss).backward()\n","            clip_grad_norm_(model.parameters(), clip_value)\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            train_loss += loss.item() * batch['input_ids'].size(0)\n","            train_preds.extend(torch.sigmoid(outputs.detach()).cpu().numpy())\n","            train_labels.extend(batch['label'].cpu().numpy())\n","\n","        # Validation phase\n","        model.eval()\n","        val_preds, val_labels = [], []\n","        val_loss = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                outputs = model(\n","                    batch['input_ids'].to(device),\n","                    batch['attention_mask'].to(device),\n","                    batch['struct_feat'].to(device)\n","                )\n","                loss = criterion(outputs, batch['label'].to(device))\n","\n","                val_loss += loss.item() * batch['input_ids'].size(0)\n","                val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","                val_labels.extend(batch['label'].cpu().numpy())\n","\n","        # Calculate metrics\n","        train_loss /= len(train_loader.dataset)\n","        val_loss /= len(val_loader.dataset)\n","\n","        train_metrics = compute_metrics(train_labels, train_preds)\n","        val_metrics = compute_metrics(val_labels, val_preds)\n","        best_threshold = find_optimal_threshold(val_labels, val_preds)\n","        val_metrics_thresh = compute_metrics(val_labels, val_preds, best_threshold)\n","\n","        # Store history\n","        history['train'].append({'loss': train_loss, **train_metrics})\n","        history['val'].append({\n","            'loss': val_loss,\n","            **val_metrics,\n","            'best_threshold': best_threshold,\n","            **val_metrics_thresh\n","        })\n","\n","        # Update scheduler\n","        scheduler.step(val_metrics['f1'])\n","\n","        # Print metrics\n","        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","        print(f\"Val AUC: {val_metrics['auc']:.4f} | Best Threshold: {best_threshold:.2f}\")\n","        print(f\"Val F1: {val_metrics_thresh['f1']:.4f} | Precision: {val_metrics_thresh['precision']:.4f} | Recall: {val_metrics_thresh['recall']:.4f}\")\n","        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        if val_metrics_thresh['f1'] > best_f1:\n","            best_f1 = val_metrics_thresh['f1']\n","            epochs_without_improvement = 0\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'threshold': best_threshold,\n","                'epoch': epoch\n","            }, \"best_model.pth\")\n","            print(\"Saved new best model!\")\n","\n","\n","    return model, history\n","\n","# 6. Evaluation Function (modified to remove image features)\n","def evaluate_model(model, loader, threshold, device):\n","    model.eval()\n","    preds, labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Evaluating\"):\n","            outputs = model(\n","                batch['input_ids'].to(device),\n","                batch['attention_mask'].to(device),\n","                batch['struct_feat'].to(device)\n","            )\n","            preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","            labels.extend(batch['label'].cpu().numpy())\n","\n","    metrics = compute_metrics(np.array(labels), np.array(preds), threshold)\n","\n","    print(f\"\\nEvaluation Results (Threshold={threshold:.2f}):\")\n","    print(f\"AUC: {metrics['auc']:.4f}\")\n","    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n","    print(f\"Precision: {metrics['precision']:.4f}\")\n","    print(f\"Recall: {metrics['recall']:.4f}\")\n","    print(f\"F1 Score: {metrics['f1']:.4f}\")\n","\n","    return metrics\n","\n","# 7. Main Execution (modified to use new model)\n","def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # Data loading\n","    df, structured_cols = load_and_preprocess_data(\"final_image_feats.pkl\")\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Data splits\n","    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['mortality_label'])\n","    train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['mortality_label'])\n","\n","    print(f\"\\nData splits:\")\n","    print(f\"Train: {len(train_df)} samples\")\n","    print(f\"Val: {len(val_df)} samples\")\n","    print(f\"Test: {len(test_df)} samples\")\n","\n","    # Datasets and DataLoaders\n","    batch_size = 16\n","    train_dataset = MultimodalDataset(train_df, structured_cols, tokenizer)\n","    val_dataset = MultimodalDataset(val_df, structured_cols, tokenizer)\n","    test_dataset = MultimodalDataset(test_df, structured_cols, tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","\n","    # Handle class imbalance\n","    class_counts = train_df['mortality_label'].value_counts()\n","    pos_weight = torch.tensor([class_counts[0] / class_counts[1]], device=device)\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","\n","    # Model and optimizer - using the new model without image features\n","    model = TextStructFusionModel(\n","        bert_model_name='bert-base-uncased',\n","        struct_dim=len(structured_cols)\n","    )\n","\n","    optimizer = AdamW([\n","        {'params': model.bert.parameters(), 'lr': 2e-5},\n","        {'params': [p for n, p in model.named_parameters() if 'bert' not in n], 'lr': 1e-4}\n","    ], weight_decay=1e-4)\n","\n","    # Training\n","    model, history = train_model(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        num_epochs=100,\n","        device=device\n","    )\n","\n","    # Evaluation\n","    checkpoint = torch.load(\"best_model.pth\", weights_only=False)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    test_metrics = evaluate_model(model, test_loader, checkpoint['threshold'], device)\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 8))\n","    metrics = ['loss', 'auc', 'f1', 'accuracy']\n","    for i, metric in enumerate(metrics, 1):\n","        plt.subplot(2, 2, i)\n","        plt.plot([x[metric] for x in history['train']], label='Train')\n","        plt.plot([x[metric] for x in history['val']], label='Val')\n","        plt.title(metric.upper())\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","    plt.tight_layout()\n","    plt.savefig('training_history.png')\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"4ab9kJfNTul4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Three features plain"],"metadata":{"id":"-cVFCYAIUa_M"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import BertModel, BertTokenizer\n","from torch.optim import AdamW\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# 1. Data Processing\n","def load_and_preprocess_data(csv_path):\n","    # Load data\n","    df = pd.read_pickle(csv_path)\n","\n","    # Convert feature lists to numpy arrays\n","    df['feat_1024'] = df['feat_1024'].apply(lambda x: np.array(x))\n","\n","    # Process structured features\n","    structured_cols = ['bun', 'calcium', 'creatinine', 'glucose', 'magnesium', 'sodium']\n","    for col in structured_cols:\n","        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n","\n","    # Standardize structured data\n","    scaler = StandardScaler()\n","    df[structured_cols] = scaler.fit_transform(df[structured_cols])\n","\n","    # Process labels\n","    df['mortality_label'] = pd.to_numeric(df['mortality_label'], errors='coerce').fillna(0).astype(int)\n","\n","    # Print class distribution\n","    class_counts = df['mortality_label'].value_counts()\n","    print(f\"Class distribution: {class_counts.to_dict()}\")\n","    print(f\"Percentage of positive samples: {class_counts.get(1, 0) / len(df) * 100:.2f}%\")\n","\n","    return df, structured_cols\n","\n","# 2. Dataset Class\n","class MultimodalDataset(Dataset):\n","    def __init__(self, df, structured_cols, tokenizer, max_length=512):\n","        self.df = df\n","        self.structured_cols = structured_cols\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Image features\n","        img_feat = torch.tensor(row['feat_1024'], dtype=torch.float32)\n","\n","        # Text features (dynamic tokenization)\n","        text = str(row['combined_note'])\n","        text_feat = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Structured features\n","        struct_data = [float(row[col]) for col in self.structured_cols]\n","        struct_feat = torch.tensor(struct_data, dtype=torch.float32)\n","\n","        # Label\n","        label = torch.tensor(float(row['mortality_label']), dtype=torch.float32)\n","\n","        return {\n","            'img_feat': img_feat,\n","            'input_ids': text_feat['input_ids'].squeeze(0),\n","            'attention_mask': text_feat['attention_mask'].squeeze(0),\n","            'struct_feat': struct_feat,\n","            'label': label\n","        }\n","\n","# 3. Model Architecture\n","class MultimodalFusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', img_dim=1024, struct_dim=6, hidden_dim=256, dropout_rate=0.3):\n","        super().__init__()\n","\n","        # BERT model (will be fine-tuned)\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        self.bert_hidden_size = self.bert.config.hidden_size\n","\n","        # Image feature processing\n","        self.img_projection = nn.Sequential(\n","            nn.Linear(img_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        # Structured data processing\n","        self.struct_projection = nn.Sequential(\n","            nn.Linear(struct_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate))\n","\n","        # Attention mechanism for feature fusion\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.bert_hidden_size + hidden_dim * 2, hidden_dim),\n","            nn.Tanh(),\n","            nn.Linear(hidden_dim, 3),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        # Final classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.bert_hidden_size + hidden_dim * 2, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, img_feat, input_ids, attention_mask, struct_feat):\n","        # BERT text processing\n","        bert_outputs = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","        text_embed = bert_outputs.last_hidden_state[:, 0, :]  # CLS token\n","\n","        # Image processing\n","        img_proj = self.img_projection(img_feat)\n","\n","        # Structured data processing\n","        struct_proj = self.struct_projection(struct_feat)\n","\n","        # Concatenate all features\n","        combined = torch.cat([text_embed, img_proj, struct_proj], dim=1)\n","\n","        # Attention-weighted fusion\n","        attention_weights = self.attention(combined)\n","        text_embed = text_embed * attention_weights[:, 0].unsqueeze(1)\n","        img_proj = img_proj * attention_weights[:, 1].unsqueeze(1)\n","        struct_proj = struct_proj * attention_weights[:, 2].unsqueeze(1)\n","\n","        # Final prediction\n","        fused = torch.cat([text_embed, img_proj, struct_proj], dim=1)\n","        output = self.classifier(fused).squeeze()\n","\n","        return output\n","\n","# 4. Training Utilities\n","def compute_metrics(y_true, y_pred, threshold=0.5):\n","    y_pred = np.array(y_pred)\n","    y_pred_bin = (y_pred >= threshold).astype(int)\n","    return {\n","        'auc': roc_auc_score(y_true, y_pred),\n","        'accuracy': accuracy_score(y_true, y_pred_bin),\n","        'precision': precision_score(y_true, y_pred_bin, zero_division=0),\n","        'recall': recall_score(y_true, y_pred_bin, zero_division=0),\n","        'f1': f1_score(y_true, y_pred_bin, zero_division=0)\n","    }\n","\n","def find_optimal_threshold(y_true, y_pred):\n","    thresholds = np.arange(0.1, 0.9, 0.05)\n","    best_threshold = 0.5\n","    best_f1 = 0\n","    for threshold in thresholds:\n","        f1 = f1_score(y_true, (y_pred >= threshold).astype(int), zero_division=0)\n","        if f1 > best_f1:\n","            best_f1 = f1\n","            best_threshold = threshold\n","    return best_threshold\n","\n","# 5. Training Loop\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device='cuda', clip_value=1.0):\n","    model = model.to(device)\n","    history = {'train': [], 'val': []}\n","    best_f1 = 0.0\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        train_preds, train_labels = [], []\n","        train_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            img_feat = batch['img_feat'].to(device)\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            struct_feat = batch['struct_feat'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(img_feat, input_ids, attention_mask, struct_feat)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            clip_grad_norm_(model.parameters(), clip_value)\n","            optimizer.step()\n","\n","            train_loss += loss.item() * img_feat.size(0)\n","            train_preds.extend(outputs.detach().cpu().numpy())\n","            train_labels.extend(labels.cpu().numpy())\n","\n","        # Validation phase\n","        model.eval()\n","        val_preds, val_labels = [], []\n","        val_loss = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                img_feat = batch['img_feat'].to(device)\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                struct_feat = batch['struct_feat'].to(device)\n","                labels = batch['label'].to(device)\n","\n","                outputs = model(img_feat, input_ids, attention_mask, struct_feat)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * img_feat.size(0)\n","                val_preds.extend(outputs.cpu().numpy())\n","                val_labels.extend(labels.cpu().numpy())\n","\n","        # Calculate metrics\n","        train_loss = train_loss / len(train_loader.dataset)\n","        val_loss = val_loss / len(val_loader.dataset)\n","\n","        train_metrics = compute_metrics(train_labels, train_preds)\n","        val_metrics = compute_metrics(val_labels, val_preds)\n","\n","        # Find optimal threshold\n","        best_threshold = find_optimal_threshold(val_labels, val_preds)\n","        val_metrics_thresh = compute_metrics(val_labels, val_preds, best_threshold)\n","\n","        # Store history\n","        history['train'].append({\n","            'loss': train_loss,\n","            **train_metrics\n","        })\n","        history['val'].append({\n","            'loss': val_loss,\n","            **val_metrics,\n","            'best_threshold': best_threshold,\n","            **val_metrics_thresh\n","        })\n","\n","        # Print metrics\n","        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","        print(f\"Val AUC: {val_metrics['auc']:.4f} | Best Threshold: {best_threshold:.2f}\")\n","        print(f\"Val Acc: {val_metrics_thresh['accuracy']:.4f} | F1: {val_metrics_thresh['f1']:.4f}\")\n","        print(f\"Precision: {val_metrics_thresh['precision']:.4f} | Recall: {val_metrics_thresh['recall']:.4f}\")\n","\n","        # Save best model\n","        if val_metrics_thresh['f1'] > best_f1:\n","            best_f1 = val_metrics_thresh['f1']\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'threshold': best_threshold,\n","                'epoch': epoch\n","            }, \"best_multimodal_model.pth\")\n","            print(\"Saved new best model!\")\n","\n","    return model, history\n","\n","# 6. Evaluation Function\n","def evaluate_model(model, loader, threshold, device):\n","    model.eval()\n","    preds, labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Evaluating\"):\n","            img_feat = batch['img_feat'].to(device)\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            struct_feat = batch['struct_feat'].to(device)\n","            labels_batch = batch['label'].to(device)\n","\n","            outputs = model(img_feat, input_ids, attention_mask, struct_feat)\n","\n","            preds.extend(outputs.cpu().numpy())\n","            labels.extend(labels_batch.cpu().numpy())\n","\n","    preds = np.array(preds)\n","    labels = np.array(labels)\n","\n","    metrics = compute_metrics(labels, preds, threshold)\n","\n","    print(f\"\\nEvaluation Results (Threshold={threshold:.2f}):\")\n","    print(f\"AUC: {metrics['auc']:.4f}\")\n","    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n","    print(f\"Precision: {metrics['precision']:.4f}\")\n","    print(f\"Recall: {metrics['recall']:.4f}\")\n","    print(f\"F1 Score: {metrics['f1']:.4f}\")\n","\n","    return metrics\n","\n","# 7. Main Execution\n","def main():\n","    # Initialize\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # Load and preprocess data\n","    df, structured_cols = load_and_preprocess_data(\"final_image_feats.pkl\")\n","\n","    # Initialize tokenizer\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Split data\n","    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['mortality_label'])\n","    train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['mortality_label'])\n","\n","    print(f\"\\nData splits:\")\n","    print(f\"Train: {len(train_df)} samples\")\n","    print(f\"Val: {len(val_df)} samples\")\n","    print(f\"Test: {len(test_df)} samples\")\n","\n","    # Create datasets\n","    train_dataset = MultimodalDataset(train_df, structured_cols, tokenizer)\n","    val_dataset = MultimodalDataset(val_df, structured_cols, tokenizer)\n","    test_dataset = MultimodalDataset(test_df, structured_cols, tokenizer)\n","\n","    # Create data loaders\n","    batch_size = 16  # Reduced for BERT fine-tuning\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Handle class imbalance\n","    class_counts = train_df['mortality_label'].value_counts()\n","    pos_weight = torch.tensor([class_counts[0] / class_counts[1]], device=device)\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","\n","    # Initialize model\n","    model = MultimodalFusionModel(\n","        bert_model_name='bert-base-uncased',\n","        img_dim=1024,\n","        struct_dim=len(structured_cols)\n","    )\n","\n","    # Initialize optimizer with different learning rates\n","    optimizer = AdamW([\n","        {'params': model.bert.parameters(), 'lr': 2e-5},\n","        {'params': [p for n, p in model.named_parameters() if 'bert' not in n], 'lr': 1e-4}\n","    ], weight_decay=1e-4)\n","\n","    # Train model\n","    model, history = train_model(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        num_epochs=100,\n","        device=device\n","    )\n","\n","    # Load best model\n","    checkpoint = torch.load(\"best_multimodal_model.pth\", weights_only=False)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    best_threshold = checkpoint['threshold']\n","\n","    # Evaluate on test set\n","    test_metrics = evaluate_model(model, test_loader, best_threshold, device)\n","\n","    # Plot training history\n","    plt.figure(figsize=(12, 8))\n","    metrics = ['loss', 'auc', 'f1', 'accuracy']\n","    for i, metric in enumerate(metrics, 1):\n","        plt.subplot(2, 2, i)\n","\n","        # Correct way to access train metrics:\n","        train_values = [epoch_stats[metric] for epoch_stats in history['train']]\n","        plt.plot(train_values, label='Train')\n","\n","        # Correct way to access val metrics:\n","        val_values = [epoch_stats[metric] for epoch_stats in history['val']]\n","        plt.plot(val_values, label='Val')\n","\n","        plt.title(metric.upper())\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","    plt.tight_layout()\n","    plt.savefig('training_history.png')\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"NZvaACLqUdQo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Three features + keyword"],"metadata":{"id":"UfHeYkLjUr7Y"}},{"cell_type":"code","source":["# Fixed imports (added missing ones)\n","import re\n","import math\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from tqdm import tqdm\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.cuda.amp\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# 1. Enhanced Data Processing - Fixed missing parenthesis and improved structure\n","def load_and_preprocess_data(csv_path):\n","    df = pd.read_pickle(csv_path)\n","    df['feat_1024'] = df['feat_1024'].apply(lambda x: np.array(x))\n","\n","    structured_cols = ['bun', 'calcium', 'creatinine', 'glucose', 'magnesium', 'sodium']\n","\n","    # Improved missing value handling\n","    for col in structured_cols:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","        df[f'{col}_missing'] = df[col].isna().astype(float)  # Missingness flags\n","        df[col] = df[col].fillna(0)\n","\n","    scaler = StandardScaler()\n","    df[structured_cols] = scaler.fit_transform(df[structured_cols])\n","\n","    # Add temporal features if available\n","    if 'charttime' in df:\n","        try:\n","            df['hour_of_day'] = pd.to_datetime(df['charttime']).dt.hour\n","            structured_cols.append('hour_of_day')\n","        except:\n","            print(\"Could not parse charttime for hour_of_day feature\")\n","\n","    df['mortality_label'] = pd.to_numeric(df['mortality_label'], errors='coerce').astype(int)\n","\n","    class_counts = df['mortality_label'].value_counts()\n","    print(f\"Class distribution: {class_counts.to_dict()}\")\n","    print(f\"Percentage of positive samples: {class_counts.get(1, 0) / len(df) * 100:.2f}%\")\n","\n","    return df, structured_cols + [f'{col}_missing' for col in structured_cols]\n","\n","# 2. Enhanced Dataset Class - Fixed regex pattern and text cleaning\n","class MultimodalDataset(Dataset):\n","    def __init__(self, df, structured_cols, tokenizer, max_length=512):\n","        self.df = df\n","        self.structured_cols = structured_cols\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.section_pattern = re.compile(\n","            r'(IMPRESSION|ASSESSMENT|DIAGNOSIS|DISCHARGE SUMMARY):(.*?)(?=\\n[A-Z]{2,}:|$)',\n","            re.IGNORECASE | re.DOTALL\n","        )\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _clean_text(self, text):\n","        text = str(text)\n","        # 1. Extract clinical sections\n","        sections = self.section_pattern.findall(text)\n","        clean_text = ' '.join([s[1].strip() for s in sections]) if sections else text\n","\n","        # 2. Remove boilerplate and de-id artifacts\n","        clean_text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', clean_text)  # Remove [** **] patterns\n","        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()  # Normalize whitespace\n","\n","        # 3. Prioritize recent info (last 2048 chars if long)\n","        return clean_text[-2048:] if len(clean_text) > 2048 else clean_text\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Image features\n","        img_feat = torch.tensor(row['feat_1024'], dtype=torch.float32)\n","\n","        # Text processing\n","        clean_text = self._clean_text(row['combined_note'])\n","        text_feat = self.tokenizer(\n","            clean_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Structured data with missingness flags\n","        struct_data = [float(row[col]) for col in self.structured_cols]\n","        struct_feat = torch.tensor(struct_data, dtype=torch.float32)\n","\n","        label = torch.tensor(float(row['mortality_label']), dtype=torch.float32)\n","\n","        return {\n","            'img_feat': img_feat,\n","            'input_ids': text_feat['input_ids'].squeeze(0),\n","            'attention_mask': text_feat['attention_mask'].squeeze(0),\n","            'struct_feat': struct_feat,\n","            'label': label\n","        }\n","\n","# 3. Model Architecture - Fixed dimension mismatches\n","class MultimodalFusionModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', img_dim=1024, struct_dim=12, hidden_dim=256, dropout_rate=0.3):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        self.bert_hidden_size = self.bert.config.hidden_size\n","\n","        self.img_projection = nn.Sequential(\n","            nn.Linear(img_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        # Fixed struct_dim to match actual input (12 = 6 labs + 6 missing flags)\n","        self.struct_projection = nn.Sequential(\n","            nn.Linear(struct_dim, hidden_dim*2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim*2, hidden_dim)\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.bert_hidden_size + hidden_dim * 2, hidden_dim),\n","            nn.Tanh(),\n","            nn.Linear(hidden_dim, 3),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.bert_hidden_size + hidden_dim * 2, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","    def forward(self, img_feat, input_ids, attention_mask, struct_feat):\n","        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        text_embed = bert_outputs.last_hidden_state[:, 0, :]\n","\n","        img_proj = self.img_projection(img_feat)\n","        struct_proj = self.struct_projection(struct_feat)\n","\n","        combined = torch.cat([text_embed, img_proj, struct_proj], dim=1)\n","        attention_weights = self.attention(combined)\n","\n","        text_embed = text_embed * attention_weights[:, 0].unsqueeze(1)\n","        img_proj = img_proj * attention_weights[:, 1].unsqueeze(1)\n","        struct_proj = struct_proj * attention_weights[:, 2].unsqueeze(1)\n","\n","        fused = torch.cat([text_embed, img_proj, struct_proj], dim=1)\n","        return self.classifier(fused).squeeze()\n","\n","# [Rest of your code remains the same...]\n","# 4. Training Utilities (unchanged)\n","def compute_metrics(y_true, y_pred, threshold=0.5):\n","    y_pred = np.array(y_pred)\n","    y_pred_bin = (y_pred >= threshold).astype(int)\n","    return {\n","        'auc': roc_auc_score(y_true, y_pred),\n","        'accuracy': accuracy_score(y_true, y_pred_bin),\n","        'precision': precision_score(y_true, y_pred_bin, zero_division=0),\n","        'recall': recall_score(y_true, y_pred_bin, zero_division=0),\n","        'f1': f1_score(y_true, y_pred_bin, zero_division=0)\n","    }\n","\n","def find_optimal_threshold(y_true, y_pred):\n","    thresholds = np.arange(0.1, 0.9, 0.05)\n","    best_threshold = 0.5\n","    best_f1 = 0\n","    for threshold in thresholds:\n","        f1 = f1_score(y_true, (y_pred >= threshold).astype(int), zero_division=0)\n","        if f1 > best_f1:\n","            best_f1 = f1\n","            best_threshold = threshold\n","    return best_threshold\n","\n","# 5. Training Loop (enhanced)\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device='cuda', clip_value=1.0):\n","    model = model.to(device)\n","    history = {'train': [], 'val': []}\n","    best_f1 = 0.0\n","    patience = 5\n","    epochs_without_improvement = 0\n","    scaler = torch.cuda.amp.GradScaler()\n","    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.1)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_preds, train_labels = [], []\n","        train_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            optimizer.zero_grad()\n","\n","            with torch.cuda.amp.autocast():\n","                outputs = model(\n","                    batch['img_feat'].to(device),\n","                    batch['input_ids'].to(device),\n","                    batch['attention_mask'].to(device),\n","                    batch['struct_feat'].to(device)\n","                )\n","                loss = criterion(outputs, batch['label'].to(device))\n","\n","            scaler.scale(loss).backward()\n","            clip_grad_norm_(model.parameters(), clip_value)\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            train_loss += loss.item() * batch['img_feat'].size(0)\n","            train_preds.extend(torch.sigmoid(outputs.detach()).cpu().numpy())\n","            train_labels.extend(batch['label'].cpu().numpy())\n","\n","        # Validation phase\n","        model.eval()\n","        val_preds, val_labels = [], []\n","        val_loss = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                outputs = model(\n","                    batch['img_feat'].to(device),\n","                    batch['input_ids'].to(device),\n","                    batch['attention_mask'].to(device),\n","                    batch['struct_feat'].to(device)\n","                )\n","                loss = criterion(outputs, batch['label'].to(device))\n","\n","                val_loss += loss.item() * batch['img_feat'].size(0)\n","                val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","                val_labels.extend(batch['label'].cpu().numpy())\n","\n","        # Calculate metrics\n","        train_loss /= len(train_loader.dataset)\n","        val_loss /= len(val_loader.dataset)\n","\n","        train_metrics = compute_metrics(train_labels, train_preds)\n","        val_metrics = compute_metrics(val_labels, val_preds)\n","        best_threshold = find_optimal_threshold(val_labels, val_preds)\n","        val_metrics_thresh = compute_metrics(val_labels, val_preds, best_threshold)\n","\n","        # Store history\n","        history['train'].append({'loss': train_loss, **train_metrics})\n","        history['val'].append({\n","            'loss': val_loss,\n","            **val_metrics,\n","            'best_threshold': best_threshold,\n","            **val_metrics_thresh\n","        })\n","\n","        # Update scheduler\n","        scheduler.step(val_metrics['f1'])\n","\n","        # Print metrics\n","        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","        print(f\"Val AUC: {val_metrics['auc']:.4f} | Best Threshold: {best_threshold:.2f}\")\n","        print(f\"Val F1: {val_metrics_thresh['f1']:.4f} | Precision: {val_metrics_thresh['precision']:.4f} | Recall: {val_metrics_thresh['recall']:.4f}\")\n","        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        if val_metrics_thresh['f1'] > best_f1:\n","            best_f1 = val_metrics_thresh['f1']\n","            epochs_without_improvement = 0\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'threshold': best_threshold,\n","                'epoch': epoch\n","            }, \"best_multimodal_model.pth\")\n","            print(\"Saved new best model!\")\n","\n","\n","    return model, history\n","\n","# 6. Evaluation Function (unchanged)\n","def evaluate_model(model, loader, threshold, device):\n","    model.eval()\n","    preds, labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Evaluating\"):\n","            outputs = model(\n","                batch['img_feat'].to(device),\n","                batch['input_ids'].to(device),\n","                batch['attention_mask'].to(device),\n","                batch['struct_feat'].to(device)\n","            )\n","            preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","            labels.extend(batch['label'].cpu().numpy())\n","\n","    metrics = compute_metrics(np.array(labels), np.array(preds), threshold)\n","\n","    print(f\"\\nEvaluation Results (Threshold={threshold:.2f}):\")\n","    print(f\"AUC: {metrics['auc']:.4f}\")\n","    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n","    print(f\"Precision: {metrics['precision']:.4f}\")\n","    print(f\"Recall: {metrics['recall']:.4f}\")\n","    print(f\"F1 Score: {metrics['f1']:.4f}\")\n","\n","    return metrics\n","\n","# 7. Main Execution (enhanced)\n","def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # Data loading\n","    df, structured_cols = load_and_preprocess_data(\"final_image_feats.pkl\")\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Data splits\n","    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['mortality_label'])\n","    train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['mortality_label'])\n","\n","    print(f\"\\nData splits:\")\n","    print(f\"Train: {len(train_df)} samples\")\n","    print(f\"Val: {len(val_df)} samples\")\n","    print(f\"Test: {len(test_df)} samples\")\n","\n","    # Datasets and DataLoaders\n","    batch_size = 16\n","    train_dataset = MultimodalDataset(train_df, structured_cols, tokenizer)\n","    val_dataset = MultimodalDataset(val_df, structured_cols, tokenizer)\n","    test_dataset = MultimodalDataset(test_df, structured_cols, tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","\n","    # Handle class imbalance\n","    class_counts = train_df['mortality_label'].value_counts()\n","    pos_weight = torch.tensor([class_counts[0] / class_counts[1]], device=device)\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","\n","    # Model and optimizer\n","    model = MultimodalFusionModel(\n","        bert_model_name='bert-base-uncased',\n","        img_dim=1024,\n","        struct_dim=len(structured_cols)\n","    )\n","\n","    optimizer = AdamW([\n","        {'params': model.bert.parameters(), 'lr': 2e-5},\n","        {'params': [p for n, p in model.named_parameters() if 'bert' not in n], 'lr': 1e-4}\n","    ], weight_decay=1e-4)\n","\n","    # Training\n","    model, history = train_model(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        num_epochs=100,\n","        device=device\n","    )\n","\n","    # Evaluation\n","    checkpoint = torch.load(\"best_multimodal_model.pth\", weights_only=False)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    test_metrics = evaluate_model(model, test_loader, checkpoint['threshold'], device)\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 8))\n","    metrics = ['loss', 'auc', 'f1', 'accuracy']\n","    for i, metric in enumerate(metrics, 1):\n","        plt.subplot(2, 2, i)\n","        plt.plot([x[metric] for x in history['train']], label='Train')\n","        plt.plot([x[metric] for x in history['val']], label='Val')\n","        plt.title(metric.upper())\n","        plt.xlabel('Epoch')\n","        plt.legend()\n","    plt.tight_layout()\n","    plt.savefig('training_history.png')\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"HaBz6y4KUuTZ"},"execution_count":null,"outputs":[]}]}