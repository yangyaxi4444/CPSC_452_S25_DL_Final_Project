{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGvCvk2LS97d"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load core ICU info\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load ICU stays\n",
        "icustays = pd.read_csv(\"icustays.csv.gz\", compression=\"gzip\", parse_dates=[\"intime\", \"outtime\"])\n",
        "icu_meta = icustays[[\"subject_id\", \"hadm_id\", \"stay_id\", \"intime\", \"first_careunit\", \"los\"]]\n",
        "\n",
        "# Load hospital admission info and assign mortality label\n",
        "admissions = pd.read_csv(\"admissions.csv.gz\", compression='gzip')\n",
        "df_mortality = admissions.merge(icustays, on=[\"subject_id\", \"hadm_id\"])\n",
        "df_mortality[\"mortality_label\"] = df_mortality[\"hospital_expire_flag\"]\n",
        "df_mortality = df_mortality[[\"subject_id\", \"hadm_id\", \"stay_id\", \"mortality_label\"]]\n",
        "\n",
        "# Step 2: Generate structured_note from icustays\n",
        "icu_meta[\"structured_note\"] = icu_meta.apply(\n",
        "    lambda row: f\"The patient was admitted to the {row['first_careunit']} and stayed for {round(row['los'], 1)} days.\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Step 3: Load procedure events\n",
        "procedures = pd.read_csv(\"procedureevents.csv.gz\", compression=\"gzip\", parse_dates=[\"starttime\"])\n",
        "\n",
        "# Step 4: Load d_items to decode procedure names\n",
        "items = pd.read_csv(\"d_items.csv.gz\", compression=\"gzip\")\n",
        "proc_map = items.set_index(\"itemid\")[\"label\"].to_dict()\n",
        "procedures[\"proc_name\"] = procedures[\"itemid\"].map(proc_map)\n",
        "\n",
        "# Step 5: Merge intime into procedures\n",
        "proc_with_time = procedures.merge(icustays[[\"subject_id\", \"stay_id\", \"intime\"]], on=[\"subject_id\", \"stay_id\"], how=\"left\")\n",
        "\n",
        "# Keep only those within 24h of ICU admission\n",
        "proc_with_time = proc_with_time[\n",
        "    (proc_with_time[\"starttime\"] >= proc_with_time[\"intime\"])\n",
        "    & (proc_with_time[\"starttime\"] <= proc_with_time[\"intime\"] + pd.Timedelta(hours=24))\n",
        "]\n",
        "\n",
        "# Step 6: Aggregate procedure descriptions\n",
        "proc_summaries = proc_with_time.groupby([\"subject_id\", \"hadm_id\", \"stay_id\"])['proc_name'].apply(lambda x: list(set(x.dropna()))).reset_index()\n",
        "\n",
        "# Turn procedure list into sentence\n",
        "proc_summaries[\"procedure_note\"] = proc_summaries[\"proc_name\"].apply(\n",
        "    lambda x: \"The patient received the following procedures within the first 24 hours: \" + \", \".join(x) + \".\" if x else \"\",\n",
        ")\n",
        "\n",
        "# Step 7: Merge all back together\n",
        "icu_enriched = icu_meta.merge(proc_summaries[[\"subject_id\", \"hadm_id\", \"stay_id\", \"procedure_note\"]],\n",
        "                              on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"left\")\n",
        "\n",
        "icu_enriched[\"procedure_note\"] = icu_enriched[\"procedure_note\"].fillna(\"\")\n",
        "\n",
        "# Step 8: Add discharge notes\n",
        "print(\"Loading discharge notes...\")\n",
        "discharge = pd.read_csv(\"discharge.csv.gz\", compression='gzip')\n",
        "discharge_latest = discharge.sort_values(\"charttime\").drop_duplicates(\"hadm_id\", keep=\"last\")\n",
        "discharge_latest[\"text_note\"] = discharge_latest[\"text\"].str.replace(r\"\\[\\*\\*.*?\\*\\*\\]\", \"\", regex=True)\n",
        "df_mortality = df_mortality.merge(discharge_latest[[\"hadm_id\", \"text_note\"]], on=\"hadm_id\", how=\"inner\")\n",
        "\n",
        "# Step 9: Add lab-based vitals from labevents\n",
        "print(\"Loading lab events for vitals...\")\n",
        "labs = pd.read_csv(\"labevents.csv.gz\", compression=\"gzip\", parse_dates=[\"charttime\"])\n",
        "\n",
        "# Example lab-based vital signs and proxies\n",
        "vital_lab_items = {\n",
        "    50912: \"glucose\",\n",
        "    50983: \"sodium\",\n",
        "    50822: \"potassium\",\n",
        "    51006: \"bun\",\n",
        "    50971: \"creatinine\",\n",
        "    50868: \"calcium\",\n",
        "    50862: \"chloride\",\n",
        "    50902: \"magnesium\",\n",
        "    50809: \"albumin\",\n",
        "    50820: \"bilirubin_total\"\n",
        "}\n",
        "\n",
        "labs = labs[labs[\"itemid\"].isin(vital_lab_items.keys())]\n",
        "labs[\"vital_label\"] = labs[\"itemid\"].map(vital_lab_items)\n",
        "# Clean value column to keep only numeric values\n",
        "labs[\"value\"] = pd.to_numeric(labs[\"value\"], errors=\"coerce\")\n",
        "labs = labs.dropna(subset=[\"value\"])\n",
        "\n",
        "# Now safely aggregate\n",
        "lab_summary = labs.groupby([\"subject_id\", \"hadm_id\", \"itemid\"]).agg({\"value\": \"median\"}).reset_index()\n",
        "\n",
        "lab_summary[\"vital_label\"] = lab_summary[\"itemid\"].map(vital_lab_items)\n",
        "\n",
        "lab_pivot = lab_summary.pivot_table(index=[\"subject_id\", \"hadm_id\"], columns=\"vital_label\", values=\"value\").reset_index()\n",
        "lab_pivot.columns.name = None\n",
        "\n",
        "# Step 10: Add CXR image path\n",
        "print(\"Loading CXR metadata...\")\n",
        "study_df = pd.read_csv(\"cxr-study-list.csv.gz\", compression=\"gzip\")\n",
        "record_df = pd.read_csv(\"cxr-record-list.csv.gz\", compression=\"gzip\")\n",
        "cxr_meta = study_df.merge(record_df, on=[\"subject_id\", \"study_id\"], how=\"inner\")\n",
        "cxr_meta[\"image_path\"] = cxr_meta[\"dicom_id\"].apply(\n",
        "    lambda x: f\"mimic-cxr/files/p{str(x)[:2]}/p{str(x)}/s{x}/{x}.jpg.gz\"\n",
        ")\n",
        "cxr_meta_dedup = cxr_meta.sort_values(\"study_id\").drop_duplicates(\"subject_id\", keep=\"first\")\n",
        "\n",
        "# Step 11: Merge everything into one final dataset\n",
        "df_full = df_mortality.merge(icu_enriched, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"left\")\n",
        "df_full = df_full.merge(lab_pivot, on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n",
        "df_full = df_full.merge(cxr_meta_dedup[[\"subject_id\", \"image_path\"]], on=\"subject_id\", how=\"inner\")\n",
        "df_full[\"combined_note\"] = df_full[\"procedure_note\"].fillna(\"\") + \" \" + df_full[\"structured_note\"].fillna(\"\") + \" \" + df_full[\"text_note\"].fillna(\"\")\n",
        "\n",
        "# Save final result\n",
        "df_full.to_csv(\"final_multimodal_dataset.csv\", index=False)\n",
        "\n",
        "# Output summary\n",
        "print(\"\\nFinal dataset columns:\")\n",
        "print(list(df_full.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any column with too much missing value\n",
        "clean_dataset = filtered_dataset.drop(columns=['potassium', 'albumin', 'chloride', 'bilirubin_total'])\n",
        "\n",
        "# Drop all rows with any missing values\n",
        "clean_dataset = clean_dataset.dropna()\n",
        "\n",
        "# Optional: check new size\n",
        "print(f\"Remaining rows after dropping missing values: {len(clean_dataset)}\")\n"
      ],
      "metadata": {
        "id": "ZkK3ej91TNCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add image information"
      ],
      "metadata": {
        "id": "nb9_Ylh2TRqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cxr_records = pd.read_csv(\"data_process/cxr-record-list.csv.gz\", compression=\"gzip\")"
      ],
      "metadata": {
        "id": "JA0opawDTQcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplicate to get one dicom_id + study_id per subject\n",
        "cxr_records_dedup = cxr_records.sort_values(\"study_id\").drop_duplicates(\"subject_id\", keep=\"first\")\n",
        "\n",
        "# Merge to bring in study_id and dicom_id\n",
        "df_merged = clean_dataset.merge(cxr_records_dedup[[\"subject_id\", \"study_id\", \"dicom_id\"]], on=\"subject_id\", how=\"inner\")\n",
        "df_merged[\"image_path\"] = df_merged.apply(\n",
        "    lambda row: f\"files/p{str(row['subject_id']).zfill(8)[:2]}/p{str(row['subject_id']).zfill(8)}/s{int(row['study_id'])}/{row['dicom_id']}.dcm\",\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "hdGeJqTnTUZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged\n",
        "print(df_merged.columns.tolist())"
      ],
      "metadata": {
        "id": "uhghtHp0TVbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.to_csv(\"final_image.csv\", index=False)"
      ],
      "metadata": {
        "id": "Rk5PJjwpTXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = df_merged.drop(columns=['study_id', 'dicom_id', 'subject_id', 'hadm_id', 'stay_id'])"
      ],
      "metadata": {
        "id": "lzCdtbygTY6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_csv(\"final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6QbFVV1gTcYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## process image features"
      ],
      "metadata": {
        "id": "9EKBYxwZam1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import torchxrayvision as xrv\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "\n",
        "# 1. Load 'final_image.csv', skipping rows with formatting issues\n",
        "df_list = pd.read_csv(\n",
        "    \"final_image.csv\",\n",
        "    dtype=str,\n",
        "    engine=\"python\",       # Use more forgiving Python parser\n",
        "    on_bad_lines=\"skip\"    # Skip problematic lines\n",
        ")\n",
        "print(f\"Loaded {len(df_list)} valid rows.\")\n",
        "print(\"Columns:\", df_list.columns.tolist())\n",
        "\n",
        "# Analyze image path column\n",
        "total_rows = len(df_list)\n",
        "total_paths = len(df_list[\"image_path\"])\n",
        "unique_paths = len(set(df_list[\"image_path\"]))\n",
        "null_paths = df_list[\"image_path\"].isna().sum()\n",
        "empty_paths = (df_list[\"image_path\"] == \"\").sum()\n",
        "\n",
        "print(f\"Total rows: {total_rows}\")\n",
        "print(f\"Total paths: {total_paths}\")\n",
        "print(f\"Unique paths: {unique_paths}\")\n",
        "print(f\"Null paths: {null_paths}\")\n",
        "print(f\"Empty paths: {empty_paths}\")\n",
        "\n",
        "# Create a mapping from path to original row indices\n",
        "path_to_indices = {}\n",
        "for idx, path in enumerate(df_list[\"image_path\"]):\n",
        "    if path not in path_to_indices:\n",
        "        path_to_indices[path] = []\n",
        "    path_to_indices[path].append(idx)\n",
        "\n",
        "# Only process unique paths but retain mapping back to original rows\n",
        "unique_paths = list(path_to_indices.keys())\n",
        "print(f\"Total unique paths to process: {len(unique_paths)}\")\n",
        "\n",
        "# 2. Set device and load pretrained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_nb\").to(device).eval()\n",
        "\n",
        "# 3. Image preprocessing pipeline (PIL → Tensor → Normalize → Grayscale)\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 4. Stream MIMIC-CXR image data from Hugging Face\n",
        "ds_stream = load_dataset(\n",
        "    \"StanfordAIMI/mimic-cxr-images-512\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# 5. Store extracted features using image path as key\n",
        "path_to_features = {}\n",
        "\n",
        "# 6. Stream and process images\n",
        "for ex in tqdm(ds_stream, total=377_110, desc=\"Scanning shards\"):\n",
        "    hf_path = ex[\"path\"]\n",
        "    hf_path_without_ext = os.path.splitext(hf_path)[0]\n",
        "\n",
        "    # Find all matching paths (ignore extension differences)\n",
        "    matching_paths = []\n",
        "    for path in list(unique_paths):  # Copy to safely remove items during iteration\n",
        "        path_without_ext = os.path.splitext(path)[0]\n",
        "        if hf_path_without_ext == path_without_ext:\n",
        "            matching_paths.append(path)\n",
        "\n",
        "    # Process each matching path\n",
        "    for path in matching_paths:\n",
        "        # Image preprocessing + feature extraction\n",
        "        img3 = transform(ex[\"image\"])\n",
        "        img1 = img3.mean(0, keepdim=True).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat = model.features(img1)\n",
        "            feat = feat.mean([2, 3]).squeeze(0)  # [1024]\n",
        "\n",
        "        # Save features\n",
        "        path_to_features[path] = feat.cpu().numpy().tolist()\n",
        "\n",
        "        # Remove processed path from the list\n",
        "        if path in unique_paths:\n",
        "            unique_paths.remove(path)\n",
        "\n",
        "        # Periodically report progress\n",
        "        if len(path_to_features) % 10 == 0:\n",
        "            print(f\"Processed {len(path_to_features)}/{total_paths} images. {len(unique_paths)} remaining.\")\n",
        "\n",
        "    # Exit early if all paths are processed\n",
        "    if not unique_paths:\n",
        "        break\n",
        "\n",
        "print(f\"Extracted features for {len(path_to_features)} images.\")\n",
        "\n",
        "# 7. Map features back to all original rows\n",
        "records = []\n",
        "for path, feat in path_to_features.items():\n",
        "    # Parse path to extract subject and study IDs\n",
        "    parts = path.split(\"/\")\n",
        "    subject_id = parts[1][1:] if parts[1].startswith('p') else parts[1]\n",
        "    study_id = parts[2][1:] if parts[2].startswith('s') else parts[2]\n",
        "\n",
        "    # For each original row associated with this path, create a full record\n",
        "    for idx in path_to_indices[path]:\n",
        "        row = df_list.iloc[idx]\n",
        "        record = {\n",
        "            \"subject_id\": subject_id,\n",
        "            \"study_id\": study_id,\n",
        "            \"image_path\": path,\n",
        "            \"feat_1024\": feat,\n",
        "        }\n",
        "\n",
        "        # Add all other columns from the original row\n",
        "        for col in df_list.columns:\n",
        "            if col != \"image_path\":  # Already added\n",
        "                record[col] = row[col]\n",
        "\n",
        "        records.append(record)\n",
        "\n",
        "print(f\"Created {len(records)} records with features.\")\n",
        "\n",
        "# 8. Save all records\n",
        "df_feats = pd.DataFrame(records)\n",
        "df_feats.to_pickle(\"final_image_feats.pkl\")\n",
        "print(f\"Saved {len(df_feats)} records to final_image_feats.pkl\")\n"
      ],
      "metadata": {
        "id": "YmS25zQeak_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}